# -*- coding: utf-8 -*-
"""Klasifikasi alexNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2EjBxtcYfc5glNJa9uG5UsFGYHeO4mO

# Preprocessing
"""

#Load dataset from kaggle

import kagglehub
path = kagglehub.dataset_download('sudarshanvaidya/random-images-for-face-emotion-recognition')

print("Path to dataset files:", path)

#import library

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping

# Path ke folder dataset
data_dir = '/root/.cache/kagglehub/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition/versions/1'

# Memuat data, secara otomatis akan menetapkan label berdasarkan nama folder
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    image_size=(224, 224),  # Ukuran gambar yang diinginkan
    batch_size=32,
    label_mode='int'  # Label akan berupa integer berdasarkan folder
)

# Mengonversi gambar ke grayscale
dataset = dataset.map(
    lambda x, y: (tf.image.rgb_to_grayscale(x), y),  # Mengubah gambar menjadi grayscale
    num_parallel_calls=tf.data.AUTOTUNE
)

# Fungsi untuk menampilkan gambar asli dan gambar setelah CLAHE
def show_original_and_clahe(original_image, clahe_image):
    """Fungsi untuk menampilkan gambar asli dan gambar setelah CLAHE"""
    plt.figure(figsize=(10, 5))

    # Menampilkan gambar asli
    plt.subplot(1, 2, 1)
    plt.title("Original Image")
    plt.imshow(original_image, cmap='gray')
    plt.axis("off")

    # Menampilkan gambar setelah CLAHE
    plt.subplot(1, 2, 2)
    plt.title("CLAHE Image")
    plt.imshow(clahe_image, cmap='gray')
    plt.axis("off")

    plt.show()

# Menyiapkan array untuk menyimpan gambar dan label
images = []
labels = []

# Iterasi pada setiap folder kelas
for class_folder in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_folder)

    if os.path.isdir(class_path):
        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)

            # Membaca gambar dalam skala abu-abu
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                # Mengubah ukuran gambar menjadi 224x224
                img_resized = cv2.resize(img, (224, 224))

                # Terapkan CLAHE
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                img_clahe = clahe.apply(img_resized)

                # Menampilkan satu contoh gambar asli dan gambar CLAHE
                show_original_and_clahe(img_resized, img_clahe)

                # Tambahkan gambar yang sudah diproses ke list
                images.append(img_clahe)
                labels.append(class_folder)  # Label adalah nama folder kelas
                break
    break

# Konversi list menjadi numpy array
images = np.array(images)
labels = np.array(labels)

#Split data
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=42,
    # image_size=(128, 128),
    batch_size=32
)

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=42,
    # image_size=(128, 128),
    batch_size=32
)

#Augmentasi
# Mendefinisikan model augmentasi
data_augmentation = Sequential([
    layers.RandomFlip("horizontal"),      # Flip horizontal
    layers.RandomRotation(0.2),           # Rotasi hingga 20%
    layers.RandomZoom(0.2),               # Zoom hingga 20%
    layers.RandomTranslation(0.1, 0.1)    # Translasi hingga 10%
])

# Terapkan augmentasi pada dataset
augmented_train_dataset = train_dataset.map(
    lambda x, y: (data_augmentation(x, training=True), y),
    num_parallel_calls=tf.data.AUTOTUNE
)

# Fungsi untuk menampilkan hasil augmentasi
def show_augmentation(original_image):
    """Fungsi untuk menampilkan hasil augmentasi pada satu gambar"""
    # Mengonversi gambar ke tensor dengan dimensi (224, 224, 1)
    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)
    original_image = tf.expand_dims(original_image, axis=-1)  # Menambahkan channel untuk grayscale

    # Melakukan augmentasi pada gambar
    augmented_images = [data_augmentation(tf.expand_dims(original_image, axis=0), training=True) for _ in range(4)]

    # Menampilkan gambar augmentasi
    plt.figure(figsize=(15, 5))
    augmented_titles = ["Zoom", "Rotation", "Flip Horizontal", "Shear (Translation)"]
    for i, aug_img in enumerate(augmented_images):
        plt.subplot(1, 4, i + 1)
        plt.title(augmented_titles[i])
        plt.imshow(aug_img[0].numpy().squeeze(), cmap='gray')  # Konversi tensor ke numpy array untuk ditampilkan
        plt.axis("off")
    plt.show()

# Contoh menampilkan hasil augmentasi untuk gambar pertama (CLAHE)
show_augmentation(images[0])  # Gambar pertama yang sudah di-preprocess

# Hitung jumlah gambar dalam train dataset sebelum augmentasi
train_size = len(augmented_train_dataset)

print(f"Jumlah gambar : {train_size}")

"""# prepro2"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split

# Path ke folder dataset asli
data_dir = '/root/.cache/kagglehub/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition/versions/1'

# Path untuk menyimpan dataset hasil CLAHE
clahe_data_dir = '/content/dataset_clahe'
os.makedirs(clahe_data_dir, exist_ok=True)

# Menerapkan CLAHE ke seluruh gambar dan menyimpannya
images = []
labels = []

for class_folder in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_folder)
    class_output_path = os.path.join(clahe_data_dir, class_folder)
    os.makedirs(class_output_path, exist_ok=True)

    if os.path.isdir(class_path):
        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                # Mengubah ukuran gambar menjadi 224x224
                img_resized = cv2.resize(img, (224, 224))

                # Terapkan CLAHE
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                img_clahe = clahe.apply(img_resized)

                # Simpan gambar yang sudah di-CLAHE ke folder baru
                clahe_img_path = os.path.join(class_output_path, img_file)
                cv2.imwrite(clahe_img_path, img_clahe)

                # Tambahkan gambar yang sudah diproses ke list
                images.append(img_clahe)
                labels.append(class_folder)

# Konversi list menjadi numpy array
images = np.array(images)
labels = np.array(labels)

# Split data menjadi train dan test (80% training, 20% testing)
train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)

# Folder untuk menyimpan hasil augmentasi
augmented_data_dir = '/content/dataset_augmented'
os.makedirs(augmented_data_dir, exist_ok=True)

# Model augmentasi
data_augmentation = Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomTranslation(0.1, 0.1)
])

# Fungsi untuk menyimpan gambar hasil augmentasi
def save_augmented_images(images, labels, output_dir, augmentation_model, num_augmentations=5):
    for i, (image, label) in enumerate(zip(images, labels)):
        label_dir = os.path.join(output_dir, label)
        os.makedirs(label_dir, exist_ok=True)

        # Convert to tensor and add dimension for augmentations
        image_tensor = tf.expand_dims(tf.convert_to_tensor(image), axis=-1)
        image_tensor = tf.image.grayscale_to_rgb(image_tensor)  # Convert to RGB for augmentations

        for j in range(num_augmentations):
            augmented_image = augmentation_model(tf.expand_dims(image_tensor, axis=0), training=True)
            aug_image_np = augmented_image[0].numpy()
            aug_image_np = cv2.cvtColor(aug_image_np, cv2.COLOR_RGB2GRAY)  # Convert back to grayscale

            # Save augmented image
            aug_img_path = os.path.join(label_dir, f"{i}_{j}.png")
            cv2.imwrite(aug_img_path, aug_image_np)

# Simpan gambar augmented untuk train dataset
save_augmented_images(train_images, train_labels, augmented_data_dir, data_augmentation)

print("Dataset preprocessing dan augmentasi selesai.")

"""# Klasifikasi"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import os

# Muat ulang dataset augmentasi sebagai tf.data.Dataset
train_augmented_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/dataset_augmented',  # Folder tempat hasil augmentasi disimpan
    image_size=(224, 224),
    color_mode="grayscale",        # Gunakan grayscale
    batch_size=32,
    label_mode='int'               # Label dalam bentuk integer
)

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    '/content/dataset_clahe',      # Folder hasil preprocessing CLAHE
    validation_split=0.2,
    subset="validation",
    seed=42,
    image_size=(224, 224),
    color_mode="grayscale",
    batch_size=32,
    label_mode='int'
)

# Definisikan model AlexNet dengan input grayscale 224x224x1
model = models.Sequential([
    layers.InputLayer(input_shape=(224, 224, 1)),

    # Convolutional Layer 1
    layers.Conv2D(96, kernel_size=(11, 11), strides=4, activation='relu'),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    # Convolutional Layer 2
    layers.Conv2D(256, kernel_size=(5, 5), padding="same", activation='relu'),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    # Convolutional Layer 3, 4, 5
    layers.Conv2D(384, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.Conv2D(384, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.Conv2D(256, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    # Flatten
    layers.Flatten(),

    # Fully Connected Layer 1
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),

    # Fully Connected Layer 2
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),

    # Output Layer (8 kelas)
    layers.Dense(8, activation='softmax')
])

# Kompilasi model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Early Stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True
)

# Melatih model dengan dataset augmentasi
history = model.fit(
    train_augmented_dataset,
    validation_data=test_dataset,
    epochs=50,
    callbacks=[early_stopping]
)

print("Pelatihan model selesai.")

# Define AlexNet model
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import os
model = models.Sequential([
    layers.Conv2D(96, kernel_size=(11, 11), strides=4, activation='relu', input_shape=(224, 224, 1)),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    layers.Conv2D(256, kernel_size=(5, 5), padding="same", activation='relu'),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    layers.Conv2D(384, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.Conv2D(384, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.Conv2D(256, kernel_size=(3, 3), padding="same", activation='relu'),
    layers.MaxPooling2D(pool_size=(3, 3), strides=2),

    layers.Flatten(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(8, activation='softmax')  # 8 classes
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',      # Monitor 'val_loss' to determine stopping
    patience=5,              # Stop if no improvement after 3 epochs
    restore_best_weights=True  # Restore the best model weights
)

# Train the model with early stopping
history = model.fit(
    augmented_train_dataset,
    validation_data=test_dataset,
    epochs=50,               # Maximum number of epochs
    callbacks=[early_stopping]  # Add early stopping to the training
)

"""# Evaluasi"""

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import numpy as np
import pandas as pd

# Plot accuracy and loss
def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Model Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Model Loss')
    plt.legend()

    plt.show()

# Call the function to plot training history
plot_training_history(history)

# Evaluasi model pada dataset validasi dan mendapatkan laporan klasifikasi
y_true = np.concatenate([y for x, y in test_dataset], axis=0)
y_pred = np.argmax(model.predict(test_dataset), axis=1)
report = classification_report(y_true, y_pred, output_dict=True)

# Convert classification report to DataFrame for plotting and display
df_report = pd.DataFrame(report).transpose()

# Plot precision, recall, and f1-score for each class as line plots
plt.figure(figsize=(10, 6))
plt.plot(df_report.iloc[:8].index, df_report.iloc[:8]['precision'], label='Precision', marker='o')
plt.plot(df_report.iloc[:8].index, df_report.iloc[:8]['recall'], label='Recall', marker='o')
plt.plot(df_report.iloc[:8].index, df_report.iloc[:8]['f1-score'], label='F1-Score', marker='o')
plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Precision, Recall, and F1-Score for Each Class')
plt.ylim(0, 1)
plt.legend(loc="upper right")
plt.grid(True)
plt.show()

# Display the classification report as a table
print("Classification Report in Table Format:")
print(df_report)

